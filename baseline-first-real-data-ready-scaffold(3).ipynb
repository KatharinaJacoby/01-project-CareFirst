{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1️⃣ Hospital & department capacity setup (UKE-focused)\n\nimport random\n\n# --- ICU units (real counts) -----------------------------------------------\n# Each ICU must reserve 1 bed for resuscitation => operational_beds = total_beds - 1\nICU_TOTAL_BEDS = {\n    \"neurochirurgical_icu\": 12,\n    \"neurological_icu\": 12,\n    \"interdis_stage1\": 12,     # UKE 1C\n    \"interdis_stage2\": 12,     # UKE 1D\n    \"interdis_stage3\": 12,     # UKE 1E\n    \"surgical_icu\": 12,        # UKE 1F\n    \"internal_medicine_icu\": 12,  # UKE 1G\n    \"cardio_icu\": 12,          # UKE H1b (cardiology)\n    \"cardio_surgery_icu\": 12,  # UKE H1b (cardiac surgery)\n    \"vascular_cardiac_icu\": 8  # UKE H2b\n}\n\nICU_OPERATIONAL_BEDS = {\n    k: max(v - 1, 0) for k, v in ICU_TOTAL_BEDS.items()\n}\n\n# --- Non-ICU departments (simulated sizes) ----------------------------------\n# Rule of thumb: Large=50, Medium=30, Small=20\nDEPARTMENTS_SIZE = {\n    # Large\n    \"internal_medicine\": 50,\n    \"neurology\": 50,\n    \"surgery_general\": 50,\n\n    # Medium\n    \"cardiology\": 30,\n    \"pulmonology\": 30,\n    \"gastroenterology\": 30,\n    \"orthopedics\": 30,\n    \"surgery_trauma\": 30,\n    \"vascular_surgery\": 30,\n\n    # Small\n    \"oncology\": 20,\n    \"pediatrics\": 20,\n    \"obstetrics_gynecology\": 20,\n    \"psychiatry\": 20,\n    \"palliative\": 20,\n    \"imc_internal_medicine\": 20,\n}\n\n# Combine into a single capacity registry (operational bed counts)\nOPER_BEDS = {\n    **ICU_OPERATIONAL_BEDS,\n    **DEPARTMENTS_SIZE\n}\n\n# --- Availability sampling ---------------------------------------------------\n# Availability is sampled per patient/event: 0%..50% free, with a small chance of 0% hard block\ndef sample_availability_fraction(hard_block_prob: float = 0.10) -> float:\n    \"\"\"Return a fraction of free capacity in [0.0, 0.5]. Sometimes forced to 0.0.\"\"\"\n    if random.random() < hard_block_prob:\n        return 0.0\n    return random.uniform(0.0, 0.5)\n\ndef capacity_snapshot_normalized(hard_block_prob: float = 0.10) -> dict:\n    \"\"\"\n    Produce a dict of normalized capacity scores per department (0..1),\n    computed as available_beds / operational_beds for the current moment.\n    \"\"\"\n    snap = {}\n    for dept, oper_beds in OPER_BEDS.items():\n        if oper_beds <= 0:\n            snap[dept] = 0.0\n            continue\n        frac = sample_availability_fraction(hard_block_prob)\n        available = int(round(oper_beds * frac))\n        snap[dept] = available / oper_beds\n    return snap\n\n# Convenience lists if needed later\nICU_DEPARTMENTS = list(ICU_OPERATIONAL_BEDS.keys())\nNON_ICU_DEPARTMENTS = list(DEPARTMENTS_SIZE.keys())\nALL_DEPARTMENTS = list(OPER_BEDS.keys())\n\n# Sanity print (optional)\nprint(\"ICU operational beds:\", ICU_OPERATIONAL_BEDS)\nprint(\"Non-ICU sizes:\", DEPARTMENTS_SIZE)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.406895Z","iopub.execute_input":"2025-08-08T11:46:01.407502Z","iopub.status.idle":"2025-08-08T11:46:01.418515Z","shell.execute_reply.started":"2025-08-08T11:46:01.407477Z","shell.execute_reply":"2025-08-08T11:46:01.417616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2️⃣ Patient generator using real ICU and simulated non-ICU capacities\n\nimport uuid\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n# Fixed suspected conditions for now\nSUSPECTED_CONDITIONS = [\n    \"cardiac\", \"respiratory\", \"neurological\",\n    \"gastrointestinal\", \"oncology\", \"infection\", \"trauma\"\n]\n\nHOME_TYPES = [\"private\", \"nursing_home\"]\n\ndef generate_patient():\n    # Static patient attributes\n    patient_id = str(uuid.uuid4())[:8]\n    age = np.random.randint(18, 95)\n    home_type = np.random.choice(HOME_TYPES)\n    suspected_condition = np.random.choice(SUSPECTED_CONDITIONS)\n    \n    # EMS and ED timing\n    ems_start_time = datetime(2025, 8, 7) + timedelta(\n        hours=np.random.randint(0, 24),\n        minutes=np.random.randint(0, 60)\n    )\n    ems_triage_code = np.random.randint(1, 5)  # 1=most urgent, 4=least\n    ems_target_hospital = \"UKE\"\n    ed_arrival_time = ems_start_time + timedelta(minutes=np.random.randint(5, 30))\n\n    # Capacity snapshot for all departments (normalized)\n    capacities = capacity_snapshot_normalized()\n\n    # Target: for now, simple suitability score for ward assignment\n    ward_assignment_suitability = np.random.uniform(0, 1)\n\n    # Combine all fields\n    record = {\n        \"patient_id\": patient_id,\n        \"age\": age,\n        \"home_type\": home_type,\n        \"suspected_condition\": suspected_condition,\n        \"ems_start_time\": ems_start_time,\n        \"ems_triage_code\": ems_triage_code,\n        \"ems_target_hospital\": ems_target_hospital,\n        \"ed_arrival_time\": ed_arrival_time,\n    }\n\n    # Add department capacities as features\n    for dept, cap in capacities.items():\n        record[f\"cap_{dept}\"] = cap\n\n    # Output target(s)\n    record[\"ward_assignment_suitability\"] = ward_assignment_suitability\n\n    return record\n\ndef generate_dataset(n_patients=500):\n    patients = [generate_patient() for _ in range(n_patients)]\n    return pd.DataFrame(patients)\n\n# Quick test\ndf = generate_dataset(5)\nprint(df.head())\nprint(\"\\nShape:\", df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.419919Z","iopub.execute_input":"2025-08-08T11:46:01.420174Z","iopub.status.idle":"2025-08-08T11:46:01.459846Z","shell.execute_reply.started":"2025-08-08T11:46:01.420155Z","shell.execute_reply":"2025-08-08T11:46:01.458903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_patient():\n    patient_id = str(uuid.uuid4())[:8]\n\n    # Demographics\n    age = np.random.randint(0, 100)\n    home_type = random.choice(['private', 'nursing_home'])\n\n    # Clinical presentation\n    suspected_condition = random.choice([\n        'cardiac', 'respiratory', 'neurological',\n        'infection', 'trauma', 'oncology', 'gastrointestinal'\n    ])\n\n    # EMS data\n    ems_start_time = datetime(2025, 8, 7) + timedelta(\n        hours=np.random.randint(0, 24),\n        minutes=np.random.randint(0, 60)\n    )\n    ems_triage_code = np.random.choice([1, 2, 3, 4], p=[0.1, 0.2, 0.4, 0.3])\n    ems_target_hospital = \"UKE\"  # fixed\n\n    # ED arrival time\n    travel_time_min = np.random.randint(5, 25)\n    ed_arrival_time = ems_start_time + timedelta(minutes=travel_time_min)\n\n    # Derived features\n    day_of_week = ed_arrival_time.strftime(\"%A\")\n    hour_of_day = ed_arrival_time.hour\n\n    # Capacities — ICU real values minus 1 bed for resus\n    icu_capacities = {\n        'neurochirurgical_icu': 11,\n        'neurological_icu': 11,\n        'interdis_stage1': 11,\n        'interdis_stage2': 11,\n        'interdis_stage3': 11,\n        'surgical_icu': 11,\n        'internal_medicine_icu': 11,\n        'cardio_icu': 11,\n        'cardio_surgery_icu': 11,\n        'vascular_cardiac_icu': 7\n    }\n\n    # Simulate ICU availability between 0–100% of operational beds\n    icu_caps = {f\"cap_{k}\": round(np.random.randint(0, beds + 1) / beds, 2)\n                for k, beds in icu_capacities.items()}\n\n    # Non-ICU sizes\n    non_icu_sizes = {\n        'internal_medicine': 50,\n        'neurology': 50,\n        'surgery_general': 50,\n        'cardiology': 30,\n        'pulmonology': 30,\n        'gastroenterology': 30,\n        'orthopedics': 30,\n        'surgery_trauma': 30,\n        'vascular_surgery': 30,\n        'oncology': 20,\n        'pediatrics': 20,\n        'obstetrics_gynecology': 20,\n        'psychiatry': 20,\n        'palliative': 20,\n        'imc_internal_medicine': 20\n    }\n\n    # Simulate availability 0–50% for non-ICU wards\n    non_icu_caps = {f\"cap_{k}\": round(np.random.randint(0, int(beds * 0.5) + 1) / beds, 2)\n                    for k, beds in non_icu_sizes.items()}\n\n    # Output variable — ground truth\n    ward_assignment_suitability = round(np.clip(np.random.normal(0.7, 0.15), 0, 1), 3)\n\n    return {\n        \"patient_id\": patient_id,\n        \"age\": age,\n        \"home_type\": home_type,\n        \"suspected_condition\": suspected_condition,\n        \"ems_start_time\": ems_start_time,\n        \"ems_triage_code\": ems_triage_code,\n        \"ems_target_hospital\": ems_target_hospital,\n        \"ed_arrival_time\": ed_arrival_time,\n        \"day_of_week\": day_of_week,\n        \"hour_of_day\": hour_of_day,\n        **icu_caps,\n        **non_icu_caps,\n        \"ward_assignment_suitability\": ward_assignment_suitability\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.461070Z","iopub.execute_input":"2025-08-08T11:46:01.461416Z","iopub.status.idle":"2025-08-08T11:46:01.486865Z","shell.execute_reply.started":"2025-08-08T11:46:01.461388Z","shell.execute_reply":"2025-08-08T11:46:01.485847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4️⃣ Generate dataset with expanded capacity features\ndef generate_dataset(n_patients=500):\n    return [generate_patient() for _ in range(n_patients)]\n\ndf = pd.DataFrame(generate_dataset(n_patients=500))\n\n# Basic hygiene\nassert df.isna().sum().sum() == 0, \"Found unexpected NaNs — check generator.\"\nassert (df[\"ems_triage_code\"].between(1, 4)).all(), \"ems_triage_code out of expected bounds.\"\n\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", list(df.columns))\n\nprint(\"\\nValue counts — suspected_condition:\")\nprint(df[\"suspected_condition\"].value_counts())\n\nprint(\"\\nValue counts — home_type:\")\nprint(df[\"home_type\"].value_counts())\n\ndisplay(df.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.540806Z","iopub.execute_input":"2025-08-08T11:46:01.541412Z","iopub.status.idle":"2025-08-08T11:46:01.729497Z","shell.execute_reply.started":"2025-08-08T11:46:01.541379Z","shell.execute_reply":"2025-08-08T11:46:01.728453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5️⃣ Preprocessing setup for expanded capacity dataset\n\n# Feature lists\nFEATS_CAT = [\"home_type\", \"suspected_condition\", \"day_of_week\"]\nFEATS_NUM = [\"age\", \"ems_triage_code\", \"hour_of_day\"] + \\\n            [c for c in df.columns if c.startswith(\"cap_\")]\n\nTARGET = \"ward_assignment_suitability\"\n\n# Split\nX = df[FEATS_CAT + FEATS_NUM]\ny = df[TARGET]\n\nX_train, X_temp, y_train, y_temp = train_test_split(\n    X, y, test_size=0.4, random_state=42\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42\n)\n\nprint(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n\n# Preprocessing pipeline\npre = ColumnTransformer([\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), FEATS_CAT),\n    (\"num\", StandardScaler(), FEATS_NUM)\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.730858Z","iopub.execute_input":"2025-08-08T11:46:01.731190Z","iopub.status.idle":"2025-08-08T11:46:01.744272Z","shell.execute_reply.started":"2025-08-08T11:46:01.731164Z","shell.execute_reply":"2025-08-08T11:46:01.743306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ridge regression sanity check\nimport time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nstart_time = time.time()\n\n# Preprocessor (re-using FEATS_CAT and FEATS_NUM from before)\npre = ColumnTransformer([\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), FEATS_CAT),\n    (\"num\", StandardScaler(), FEATS_NUM)\n])\n\nmodel = Pipeline([\n    (\"pre\", pre),\n    (\"reg\", Ridge(alpha=1.0))\n])\n\n# Train\nmodel.fit(X_train, y_train)\n\n# Validate\ny_val_pred = model.predict(X_val)\nmse = mean_squared_error(y_val, y_val_pred)\nmae = mean_absolute_error(y_val, y_val_pred)\n\nend_time = time.time()\n\nprint(f\"Validation MSE: {mse:.4f}\")\nprint(f\"Validation MAE: {mae:.4f}\")\nprint(f\"Training time: {end_time - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.745108Z","iopub.execute_input":"2025-08-08T11:46:01.745328Z","iopub.status.idle":"2025-08-08T11:46:01.778943Z","shell.execute_reply.started":"2025-08-08T11:46:01.745311Z","shell.execute_reply":"2025-08-08T11:46:01.778107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔹 Step 1: Preprocess with expanded feature set\nFEATS_CAT = [\"home_type\", \"suspected_condition\", \"day_of_week\"]\nFEATS_NUM = [\n    \"age\", \"ems_triage_code\", \"hour_of_day\",\n    # All capacity columns\n] + [col for col in df.columns if col.startswith(\"cap_\")]\n\nTARGET = \"ward_assignment_suitability\"\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\npre = ColumnTransformer([\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), FEATS_CAT),\n    (\"num\", StandardScaler(), FEATS_NUM)\n])\n\nX_train_np = pre.fit_transform(X_train)\nX_val_np   = pre.transform(X_val)\nX_test_np  = pre.transform(X_test)\n\ny_train_np = y_train.values.astype(\"float32\")\ny_val_np   = y_val.values.astype(\"float32\")\ny_test_np  = y_test.values.astype(\"float32\")\n\nprint(\"Shapes:\", X_train_np.shape, X_val_np.shape, X_test_np.shape)\n\n# 🔹 Step 2: Convert to torch tensors and loaders\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nXtr = torch.tensor(X_train_np, dtype=torch.float32)\nytr = torch.tensor(y_train_np.reshape(-1, 1), dtype=torch.float32)\nXva = torch.tensor(X_val_np, dtype=torch.float32)\nyva = torch.tensor(y_val_np.reshape(-1, 1), dtype=torch.float32)\nXte = torch.tensor(X_test_np, dtype=torch.float32)\nyte = torch.tensor(y_test_np.reshape(-1, 1), dtype=torch.float32)\n\ntrain_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=64, shuffle=True)\nval_loader   = DataLoader(TensorDataset(Xva, yva), batch_size=256)\ntest_loader  = DataLoader(TensorDataset(Xte, yte), batch_size=256)\n\ninput_dim = Xtr.shape[1]\nprint(\"Input dim:\", input_dim)\n\n# 🔹 Step 3: Define the MLP\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass MLP(nn.Module):\n    def __init__(self, d_in):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_in, 128), nn.ReLU(), nn.Dropout(0.1),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nmlp = MLP(input_dim)\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n\n# 🔹 Step 4: Training loop with early stopping\nbest_val = np.inf\npatience, bad = 8, 0\nepochs = 50\n\nfor ep in range(1, epochs + 1):\n    mlp.train()\n    train_loss = 0.0\n    for xb, yb in train_loader:\n        optimizer.zero_grad()\n        pred = mlp(xb)\n        loss = criterion(pred, yb)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * xb.size(0)\n    train_loss /= len(train_loader.dataset)\n\n    mlp.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            val_loss += criterion(mlp(xb), yb).item() * xb.size(0)\n    val_loss /= len(val_loader.dataset)\n\n    print(f\"Epoch {ep:02d} | train MSE {train_loss:.4f} | val MSE {val_loss:.4f}\")\n\n    if val_loss + 1e-6 < best_val:\n        best_val = val_loss\n        bad = 0\n        best_state = {k: v.cpu().clone() for k, v in mlp.state_dict().items()}\n    else:\n        bad += 1\n        if bad >= patience:\n            print(\"Early stopping.\")\n            break\n\nmlp.load_state_dict(best_state)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:46:01.780491Z","iopub.execute_input":"2025-08-08T11:46:01.780792Z","iopub.status.idle":"2025-08-08T11:46:02.057621Z","shell.execute_reply.started":"2025-08-08T11:46:01.780772Z","shell.execute_reply":"2025-08-08T11:46:02.056868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test set evaluation\nmlp.eval()\nwith torch.no_grad():\n    y_pred_test = np.vstack([mlp(xb).cpu().numpy() for xb, _ in test_loader]).ravel()\n\nmse_test = mean_squared_error(y_test_np, y_pred_test)\nmae_test = mean_absolute_error(y_test_np, y_pred_test)\n\nprint(f\"MLP Test MSE: {mse_test:.4f} | MAE: {mae_test:.4f}\")\n\n# Scatter plot\nplt.figure(figsize=(6,6))\nsns.scatterplot(x=y_test_np, y=y_pred_test, alpha=0.7)\nplt.plot([0,1], [0,1], 'r--', label=\"Perfect\")\nplt.xlabel(\"Actual suitability\")\nplt.ylabel(\"Predicted suitability\")\nplt.title(\"MLP — Predicted vs Actual Suitability\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:47:56.776101Z","iopub.execute_input":"2025-08-08T11:47:56.776471Z","iopub.status.idle":"2025-08-08T11:47:57.025795Z","shell.execute_reply.started":"2025-08-08T11:47:56.776448Z","shell.execute_reply":"2025-08-08T11:47:57.024631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell PI-1 — Get feature names + define a predict helper\nimport numpy as np\n\n# Recover feature names from the fitted preprocessor used for X_*_np\ncat_names = pre.named_transformers_[\"cat\"].get_feature_names_out(FEATS_CAT)\nnum_names = np.array(FEATS_NUM, dtype=str)\nfeature_names = np.concatenate([cat_names, num_names])\n\nassert feature_names.shape[0] == X_test_np.shape[1], (\n    f\"Name/feature mismatch: {feature_names.shape[0]} vs {X_test_np.shape[1]}\"\n)\n\n# Torch → numpy prediction helper on raw numpy features\nimport torch\n\ndef mlp_predict_numpy(X_np: np.ndarray) -> np.ndarray:\n    X_tensor = torch.tensor(X_np, dtype=torch.float32)\n    mlp.eval()\n    with torch.no_grad():\n        y_hat = mlp(X_tensor).cpu().numpy().ravel()\n    return y_hat\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:11:52.138716Z","iopub.execute_input":"2025-08-08T12:11:52.139142Z","iopub.status.idle":"2025-08-08T12:11:52.148728Z","shell.execute_reply.started":"2025-08-08T12:11:52.139115Z","shell.execute_reply":"2025-08-08T12:11:52.147523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell PI-2 — Permutation importance (on test set)\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef permutation_importance_torch(predict_fn, X, y, n_repeats=10, seed=42):\n    rng = np.random.default_rng(seed)\n    baseline = mean_squared_error(y, predict_fn(X))\n    importances = np.zeros(X.shape[1], dtype=float)\n\n    for j in range(X.shape[1]):\n        losses = []\n        X_perm = X.copy()\n        for _ in range(n_repeats):\n            rng.shuffle(X_perm[:, j])   # in-place column shuffle\n            y_hat = predict_fn(X_perm)\n            losses.append(mean_squared_error(y, y_hat))\n        importances[j] = np.mean(losses) - baseline\n    return importances, baseline\n\nimps, base_mse = permutation_importance_torch(mlp_predict_numpy, X_test_np, y_test_np, n_repeats=20)\n\nimp_df = pd.DataFrame({\n    \"feature\": feature_names,\n    \"perm_mse_increase\": imps\n}).sort_values(\"perm_mse_increase\", ascending=False)\n\nprint(f\"Baseline test MSE: {base_mse:.4f}\")\ndisplay(imp_df.head(20))\n\n# Plot top 15\ntop = imp_df.head(15).iloc[::-1]  # reverse for nicer barh\nplt.figure(figsize=(8, 6))\nplt.barh(top[\"feature\"], top[\"perm_mse_increase\"])\nplt.xlabel(\"MSE increase when permuted\")\nplt.title(\"Permutation Importance (Test set) — Top 15 features\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:12:24.569386Z","iopub.execute_input":"2025-08-08T12:12:24.570634Z","iopub.status.idle":"2025-08-08T12:12:25.343399Z","shell.execute_reply.started":"2025-08-08T12:12:24.570590Z","shell.execute_reply":"2025-08-08T12:12:25.342444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell PI-3 — Quick grouping summary\n# This helps see ICU if capacities matter more than non-ICU.\n\nimp_df[\"group\"] = np.where(\n    imp_df[\"feature\"].str.contains(\"cap_\"),\n    np.where(\n        imp_df[\"feature\"].str.contains(\"cap_(neurochirurgical_icu|neurological_icu|interdis_stage|surgical_icu|internal_medicine_icu|cardio_icu|cardio_surgery_icu|vascular_cardiac_icu)\"),\n        \"ICU capacity\",\n        \"Non-ICU capacity\"\n    ),\n    \"Other\"\n)\n\ngroup_summary = imp_df.groupby(\"group\")[\"perm_mse_increase\"].sum().reset_index().sort_values(\"perm_mse_increase\", ascending=False)\ndisplay(group_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:14:04.847938Z","iopub.execute_input":"2025-08-08T12:14:04.849073Z","iopub.status.idle":"2025-08-08T12:14:04.872673Z","shell.execute_reply.started":"2025-08-08T12:14:04.849040Z","shell.execute_reply":"2025-08-08T12:14:04.871635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1️⃣ Identify ICU capacity features\nicu_features = [\n    'cap_neurochirurgical_icu',\n    'cap_neurological_icu',\n    'cap_interdis_stage1',\n    'cap_interdis_stage2',\n    'cap_interdis_stage3',\n    'cap_surgical_icu',\n    'cap_internal_medicine_icu',\n    'cap_cardio_icu',\n    'cap_cardio_surgery_icu',\n    'cap_vascular_cardiac_icu'\n]\n\n# 2️⃣ Correlation with target (train split only to avoid test leakage)\ncorrelations = {}\nfor feat in icu_features:\n    correlations[feat] = X_train[feat].corr(y_train)\n\n# Sort by absolute correlation\ncor_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Pearson_corr'])\ncor_df['abs_corr'] = cor_df['Pearson_corr'].abs()\ncor_df.sort_values('abs_corr', ascending=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:34:21.416099Z","iopub.execute_input":"2025-08-08T12:34:21.416426Z","iopub.status.idle":"2025-08-08T12:34:21.434897Z","shell.execute_reply.started":"2025-08-08T12:34:21.416403Z","shell.execute_reply":"2025-08-08T12:34:21.434001Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                           Pearson_corr  abs_corr\ncap_interdis_stage2           -0.185904  0.185904\ncap_cardio_surgery_icu        -0.100707  0.100707\ncap_neurochirurgical_icu       0.095247  0.095247\ncap_surgical_icu               0.092119  0.092119\ncap_vascular_cardiac_icu      -0.063671  0.063671\ncap_cardio_icu                -0.042115  0.042115\ncap_interdis_stage1           -0.030689  0.030689\ncap_interdis_stage3            0.027915  0.027915\ncap_internal_medicine_icu     -0.007139  0.007139\ncap_neurological_icu           0.006564  0.006564","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pearson_corr</th>\n      <th>abs_corr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cap_interdis_stage2</th>\n      <td>-0.185904</td>\n      <td>0.185904</td>\n    </tr>\n    <tr>\n      <th>cap_cardio_surgery_icu</th>\n      <td>-0.100707</td>\n      <td>0.100707</td>\n    </tr>\n    <tr>\n      <th>cap_neurochirurgical_icu</th>\n      <td>0.095247</td>\n      <td>0.095247</td>\n    </tr>\n    <tr>\n      <th>cap_surgical_icu</th>\n      <td>0.092119</td>\n      <td>0.092119</td>\n    </tr>\n    <tr>\n      <th>cap_vascular_cardiac_icu</th>\n      <td>-0.063671</td>\n      <td>0.063671</td>\n    </tr>\n    <tr>\n      <th>cap_cardio_icu</th>\n      <td>-0.042115</td>\n      <td>0.042115</td>\n    </tr>\n    <tr>\n      <th>cap_interdis_stage1</th>\n      <td>-0.030689</td>\n      <td>0.030689</td>\n    </tr>\n    <tr>\n      <th>cap_interdis_stage3</th>\n      <td>0.027915</td>\n      <td>0.027915</td>\n    </tr>\n    <tr>\n      <th>cap_internal_medicine_icu</th>\n      <td>-0.007139</td>\n      <td>0.007139</td>\n    </tr>\n    <tr>\n      <th>cap_neurological_icu</th>\n      <td>0.006564</td>\n      <td>0.006564</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# 1️⃣ Reduced feature set without ICU\nFEATS_CAT_no_icu = [f for f in FEATS_CAT if not f.startswith('cap_')]  # categorical unaffected\nFEATS_NUM_no_icu = [f for f in FEATS_NUM if f not in icu_features]      # drop ICU from numeric\n\n# Combine\nX_train_no_icu = X_train.drop(columns=icu_features)\nX_val_no_icu   = X_val.drop(columns=icu_features)\nX_test_no_icu  = X_test.drop(columns=icu_features)\n\n# 2️⃣ Preprocess\npre_no_icu = ColumnTransformer([\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), FEATS_CAT),\n    (\"num\", StandardScaler(), FEATS_NUM_no_icu)\n])\n\nX_train_np_no_icu = pre_no_icu.fit_transform(X_train_no_icu)\nX_val_np_no_icu   = pre_no_icu.transform(X_val_no_icu)\nX_test_np_no_icu  = pre_no_icu.transform(X_test_no_icu)\n\n# 3️⃣ Torch conversion\nXtr_no_icu = torch.tensor(X_train_np_no_icu, dtype=torch.float32)\nytr_no_icu = torch.tensor(y_train.values.reshape(-1,1), dtype=torch.float32)\nXva_no_icu = torch.tensor(X_val_np_no_icu,   dtype=torch.float32)\nyva_no_icu = torch.tensor(y_val.values.reshape(-1,1),   dtype=torch.float32)\nXte_no_icu = torch.tensor(X_test_np_no_icu,  dtype=torch.float32)\nyte_no_icu = torch.tensor(y_test.values.reshape(-1,1),  dtype=torch.float32)\n\ntrain_loader_no_icu = DataLoader(TensorDataset(Xtr_no_icu, ytr_no_icu), batch_size=64, shuffle=True)\nval_loader_no_icu   = DataLoader(TensorDataset(Xva_no_icu, yva_no_icu), batch_size=256)\ntest_loader_no_icu  = DataLoader(TensorDataset(Xte_no_icu, yte_no_icu), batch_size=256)\n\n# 4️⃣ New MLP\nmlp_no_icu = MLP(Xtr_no_icu.shape[1])\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(mlp_no_icu.parameters(), lr=1e-3)\n\n# 5️⃣ Training loop\nbest_val = np.inf\npatience, bad = 8, 0\nepochs = 50\nfor ep in range(1, epochs+1):\n    mlp_no_icu.train(); train_loss = 0.0\n    for xb, yb in train_loader_no_icu:\n        optimizer.zero_grad()\n        pred = mlp_no_icu(xb)\n        loss = criterion(pred, yb)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * xb.size(0)\n    train_loss /= len(train_loader_no_icu.dataset)\n\n    mlp_no_icu.eval(); val_loss = 0.0\n    with torch.no_grad():\n        for xb, yb in val_loader_no_icu:\n            val_loss += criterion(mlp_no_icu(xb), yb).item() * xb.size(0)\n    val_loss /= len(val_loader_no_icu.dataset)\n\n    if val_loss + 1e-6 < best_val:\n        best_val = val_loss; bad = 0\n        best_state = {k: v.cpu().clone() for k,v in mlp_no_icu.state_dict().items()}\n    else:\n        bad += 1\n        if bad >= patience:\n            break\n\nmlp_no_icu.load_state_dict(best_state)\n\n# 6️⃣ Test performance\nmlp_no_icu.eval()\nwith torch.no_grad():\n    y_pred_no_icu = np.vstack([mlp_no_icu(xb).cpu().numpy() for xb, _ in test_loader_no_icu]).ravel()\n\nmse_test_no_icu = mean_squared_error(y_test, y_pred_no_icu)\nmae_test_no_icu = mean_absolute_error(y_test, y_pred_no_icu)\nprint(f\"No ICU features → Test MSE: {mse_test_no_icu:.4f} | MAE: {mae_test_no_icu:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:36:32.778084Z","iopub.execute_input":"2025-08-08T12:36:32.778440Z","iopub.status.idle":"2025-08-08T12:36:33.092817Z","shell.execute_reply.started":"2025-08-08T12:36:32.778408Z","shell.execute_reply":"2025-08-08T12:36:33.091622Z"}},"outputs":[{"name":"stdout","text":"No ICU features → Test MSE: 0.0262 | MAE: 0.1386\n","output_type":"stream"}],"execution_count":36}]}